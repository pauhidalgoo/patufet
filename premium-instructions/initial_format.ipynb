{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def format_instructions(text):\n",
    "    lines = text.split('\\n')\n",
    "    if lines[0].startswith('##'):\n",
    "        lines = lines[1:]  # Remove the first line\n",
    "\n",
    "    text = '\\n'.join(lines)\n",
    "\n",
    "    text = re.sub(r'Usuari:', 'User:', text)\n",
    "    text = re.sub(r'IA:', 'AI:', text)\n",
    "\n",
    "    text = re.sub(r'>*\\d+\\.\\s*User:\\s*', 'User: ', text)\n",
    "    text = re.sub(r'\\**User:\\s*\\**', 'User: ', text)\n",
    "    text = re.sub(r'\\**AI:\\s*\\**', 'AI: ', text) \n",
    "\n",
    "    text = re.sub(r'---', '', text) \n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('revised_instructs_ultra.csv')\n",
    "\n",
    "# Apply the format_instructions function to the 'Instructions' column\n",
    "df['Instructions'] = df['Instructions'].apply(format_instructions)\n",
    "\n",
    "# Save the formatted DataFrame back to a CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4535, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def drop_half(df, name):\n",
    "    list_gen_entries = df[df['Type'] == name]\n",
    "    num_to_drop = len(list_gen_entries) // 2\n",
    "    indices_to_drop = np.random.choice(list_gen_entries.index, size=num_to_drop, replace=False)\n",
    "    df = df.drop(indices_to_drop)\n",
    "    return df\n",
    "\n",
    "df = drop_half(df, 'List generation')\n",
    "df = drop_half(df, 'Open QA')\n",
    "df = drop_half(df, 'Format Following')\n",
    "df = drop_half(df, 'Learning and Educational Resources')\n",
    "df = drop_half(df, 'Step-by-Step Guidance')\n",
    "df = drop_half(df, 'Brainstorming')\n",
    "df = drop_half(df, 'Meta Reasoning')\n",
    "df = drop_half(df, 'Specific Constraints')\n",
    "df = drop_half(df, 'Plan Creation')\n",
    "df = drop_half(df, 'Generation')\n",
    "df = drop_half(df, 'Question Generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4164, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('revised_instructs2.csv')  # The dataset from which you'll select entries\n",
    "\n",
    "# Define the list of types to keep\n",
    "types_to_keep = ['Explanation', 'Time-Bound', 'Math World Problems', 'Factual Recall', \n",
    "                 'Chat', 'Translation', 'Creative writing', 'Code Generation', \n",
    "                 'Closed QA', 'Semantics Questions', 'Jokes and Riddles']\n",
    "\n",
    "# Filter df2 for rows with these types\n",
    "filtered_df2 = df2[df2['Type'].isin(types_to_keep)]\n",
    "\n",
    "# Handle \"Generation\" type separately, selecting half of those rows randomly\n",
    "generation_entries = df2[df2['Type'] == 'Generation']\n",
    "num_to_add = len(generation_entries) // 2  # Select half of the \"Generation\" entries\n",
    "generation_sample = generation_entries.sample(n=num_to_add, random_state=42)  # Random sample\n",
    "\n",
    "# Concatenate the filtered df2 (with \"types_to_keep\") and half of the \"Generation\" entries\n",
    "final_filtered_df2 = pd.concat([filtered_df2, generation_sample])\n",
    "\n",
    "# Append the filtered entries from df2 to df1\n",
    "combined_df = pd.concat([df, final_filtered_df2], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('combined_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('combined_dataset.csv')\n",
    "\n",
    "# Function to split interactions into Prompt (User) and Completion (AI)\n",
    "def split_interactions(text):\n",
    "    interactions = re.split(r'(User:|AI:)', text)  # Split by User: and AI:\n",
    "    user_ai_pairs = []\n",
    "    current_prompt = \"\"\n",
    "    current_completion = \"\"\n",
    "    \n",
    "    for i in range(1, len(interactions), 2):\n",
    "        if interactions[i].strip() == 'User:':\n",
    "            if current_prompt and current_completion:\n",
    "                user_ai_pairs.append([current_prompt.strip(), current_completion.strip()])\n",
    "            current_prompt = interactions[i + 1].strip()\n",
    "            current_completion = \"\"\n",
    "        elif interactions[i].strip() == 'AI:':\n",
    "            current_completion = interactions[i + 1].strip()\n",
    "\n",
    "    # Add the last pair if exists\n",
    "    if current_prompt and current_completion:\n",
    "        user_ai_pairs.append([current_prompt.strip(), current_completion.strip()])\n",
    "\n",
    "    return user_ai_pairs\n",
    "\n",
    "# Create a new DataFrame to store the split interactions\n",
    "new_data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    interactions = split_interactions(row['Instructions'])\n",
    "    for prompt, completion in interactions:\n",
    "        new_row = row.copy()  # Copy the original row to keep other columns the same\n",
    "        new_row['prompt'] = prompt\n",
    "        new_row['completion'] = completion\n",
    "        new_data.append(new_row)\n",
    "\n",
    "# Create a new DataFrame with the split interactions\n",
    "new_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Drop the original 'Instructions' column as we now have 'Prompt (User)' and 'Completion (AI)'\n",
    "new_df = new_df.drop(columns=['Instructions'])\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_df.to_csv('prompts_instructs_ultra.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna(subset=['prompt'])\n",
    "new_df = new_df.dropna(subset=['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6625, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n",
      "one found\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "new_data = []\n",
    "seen_prompts = []\n",
    "seen_completions = []\n",
    "\n",
    "def is_similar(text1, text2, threshold=90):\n",
    "    \"\"\"\n",
    "    Compare two strings using fuzzy matching.\n",
    "    If the similarity score is greater than the threshold, consider them duplicates.\n",
    "    \"\"\"\n",
    "    return fuzz.ratio(text1, text2) > threshold\n",
    "\n",
    "for index, row in new_df.iterrows():\n",
    "    prompt = row['prompt']\n",
    "    completion = row['completion']\n",
    "    is_duplicate = False\n",
    "    for seen_prompt, seen_completion in zip(seen_prompts, seen_completions):\n",
    "        if is_similar(prompt, seen_prompt) and is_similar(completion, seen_completion):\n",
    "            is_duplicate = True\n",
    "            print(\"one found\")\n",
    "            break\n",
    "    if not is_duplicate:\n",
    "        new_row = row.copy()\n",
    "        new_data.append(new_row)\n",
    "        seen_prompts.append(prompt)\n",
    "        seen_completions.append(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('prompts_instructs_dedup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository, DatasetCard, DatasetCardData\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8440f52621144a3ad6e3386bb91694b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"csv\", data_files=\"prompts_instructs_ultra_revised.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Type': 'Multiple-Choice',\n",
       " 'Words': \"['dualism', 'sun', 'studies', 'success', 'space']\",\n",
       " 'prompt': \"Durant l'Imperi Romà, quina d'aquestes ciutats NO estava situada a la península Ibèrica?\\nA)  Tarragona\\nB)  Cartago\\nC)  Sagunt\\nD)  Empúries\",\n",
       " 'completion': 'B)  Cartago'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset['train'][20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = hf_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (41176, 4)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccccccd51ff4050b5aec01ec4b63f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a879542340406497e75784b5e539cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41343653f94e466ca2b452b141f42f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/386 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\datasets--pauhidalgoo--patufet-premium-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded successfully to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "hf_dataset.push_to_hub(\"pauhidalgoo/patufet-premium-instruct\")\n",
    "\n",
    "print(\"Dataset uploaded successfully to Hugging Face!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
